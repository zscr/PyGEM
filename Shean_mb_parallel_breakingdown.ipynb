{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate a 3-panel plot for input arrays\n",
    "def plot_array(dem, clim=None, titles=None, cmap='inferno', label=None, overlay=None, fn=None):\n",
    "    fig, ax = plt.subplots(1,1, sharex=True, sharey=True, figsize=(10,5))\n",
    "    alpha = 1.0\n",
    "    #Gray background\n",
    "    ax.set_facecolor('0.5')\n",
    "    #Force aspect ratio to match images\n",
    "    ax.set(aspect='equal')\n",
    "    #Turn off axes labels/ticks\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if titles is not None:\n",
    "        ax.set_title(titles[0])\n",
    "    #Plot background shaded relief map\n",
    "    if overlay is not None:\n",
    "        alpha = 0.7\n",
    "        ax.imshow(overlay, cmap='gray', clim=(1,255))\n",
    "    #Plot each array\n",
    "    im_list = [ax.imshow(dem, clim=clim, cmap=cmap, alpha=alpha)]\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(im_list[0], label=label, extend='both', shrink=0.5)\n",
    "    if fn is not None:\n",
    "        fig.savefig(fn, bbox_inches='tight', pad_inches=0, dpi=150)\n",
    "\n",
    "#Function to generate a 3-panel plot for input arrays\n",
    "def plot2panel(dem_list, clim=None, titles=None, cmap='inferno', label=None, overlay=None, fn=None):\n",
    "    fig, axa = plt.subplots(1,2, sharex=True, sharey=True, figsize=(10,5))\n",
    "    alpha = 1.0\n",
    "    for n, ax in enumerate(axa):\n",
    "        #Gray background\n",
    "        ax.set_facecolor('0.5')\n",
    "        #Force aspect ratio to match images\n",
    "        ax.set(aspect='equal')\n",
    "        #Turn off axes labels/ticks\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if titles is not None:\n",
    "            ax.set_title(titles[n])\n",
    "        #Plot background shaded relief map\n",
    "        if overlay is not None:\n",
    "            alpha = 0.7\n",
    "            axa[n].imshow(overlay[n], cmap='gray', clim=(1,255))\n",
    "    #Plot each array\n",
    "    im_list = [axa[i].imshow(dem_list[i], clim=clim, cmap=cmap, alpha=alpha) for i in range(len(dem_list))]\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(im_list[0], ax=axa.ravel().tolist(), label=label, extend='both', shrink=0.5)\n",
    "    if fn is not None:\n",
    "        fig.savefig(fn, bbox_inches='tight', pad_inches=0, dpi=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warping all inputs to the following:\n",
      "Resolution: 30.0\n",
      "Extent: [537155.67713055, 6545401.2408929, 695675.67713055, 6657601.2408929]\n",
      "Projection: '+proj=utm +zone=7 +datum=WGS84 +units=m +no_defs '\n",
      "Resampling alg: cubic\n",
      "\n",
      "1 of 2: /Users/davidrounce/Documents/Dave_Rounce/HiMAT/DEMs/Braun/TDX-SRTM_prelim/NorthAmerica_dhdt/region_03_PCRn/srtm_filled_ice__03_PCRn-utm07N-strips_crp2reg_03_PCRn.tif\n",
      "nl: 3740 ns: 5284 res: 30.000\n",
      "2 of 2: /Users/davidrounce/Documents/Dave_Rounce/HiMAT/DEMs/Braun/TDX-SRTM_prelim/NorthAmerica_dhdt/region_03_PCRn/dh_dt_on_ice__03_PCRn-utm07N-strips_crp2reg_03_PCRn.tif\n",
      "\n",
      "Static analysis does not work for quantifying uncertainty because clipped by RGI extents\n",
      "\n",
      "Opting to use UTM projections to avoid errors caused by projecting/resampling datasets\n",
      "\n",
      "Shp init crs: {'init': 'epsg:4326'}\n",
      "Input glacier polygon count: 27108\n",
      "Glacier polygon count after spatial filter: 1156\n",
      "Min. Area filter glacier polygon count: 1156\n",
      "Processing 1156 features\n",
      "Loading /Users/davidrounce/Documents/Dave_Rounce/HiMAT/DEMs/Braun/output/Braun_PCR07N_glacfeat_list.p\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\"\"\"\n",
    "Compute dh/dt and mass balance for input DEMs and glacier polygons\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Todo:\n",
    "GDAL_MAX_DATASET_POOL_SIZE - set to large number of open datasets in vrt\n",
    "Better error estimates - use buffered dz/dt and semivariogram\n",
    "Filling gaps using 1) dz/dt obs 2) setting to 0 around polygon margins\n",
    "Curves for PRISM T an precip vs. mb\n",
    "Move mb_plot_gpd funcitonality here, export polygons with mb numbers as geojson, spatialite, shp?\n",
    "Add +/- std for each dh/dt polygon, some idea of spread\n",
    "Create main function, pass args to mb_proc\n",
    "Clean up mb_proc function, one return, globals\n",
    "Better penetration correction\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "from pygeotools.lib import malib, warplib, geolib, iolib, timelib\n",
    "# from imview.lib import pltlib\n",
    "\n",
    "#Avoid printing out divide by 0 errors\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "\"\"\"\n",
    "Class to store relevant feature attributes and derived values\n",
    "Safe for multiprocessing\n",
    "\"\"\"\n",
    "class GlacFeat:\n",
    "    def __init__(self, feat, glacname_fieldname, glacnum_fieldname):\n",
    "\n",
    "        self.glacname = feat.GetField(glacname_fieldname)\n",
    "        if self.glacname is None:\n",
    "            self.glacname = \"\"\n",
    "        else:\n",
    "            #RGI has some nonstandard characters\n",
    "            #self.glacname = self.glacname.decode('unicode_escape').encode('ascii','ignore')\n",
    "            #glacname = re.sub(r'[^\\x00-\\x7f]',r'', glacname)\n",
    "            self.glacname = re.sub(r'\\W+', '', self.glacname)\n",
    "            self.glacname = self.glacname.replace(\" \", \"\")\n",
    "            self.glacname = self.glacname.replace(\"_\", \"\")\n",
    "            self.glacname = self.glacname.replace(\"/\", \"\")\n",
    "\n",
    "        self.glacnum = feat.GetField(glacnum_fieldname)\n",
    "        fn = feat.GetDefnRef().GetName()\n",
    "        #RGIId (String) = RGI50-01.00004\n",
    "        self.glacnum = '%0.5f' % float(self.glacnum.split('-')[-1])\n",
    "\n",
    "        if self.glacname:\n",
    "            self.feat_fn = \"%s_%s\" % (self.glacnum, self.glacname)\n",
    "        else:\n",
    "            self.feat_fn = str(self.glacnum)\n",
    "\n",
    "        self.glac_geom_orig = geolib.geom_dup(feat.GetGeometryRef())\n",
    "        self.glac_geom = geolib.geom_dup(self.glac_geom_orig)\n",
    "        #Hack to deal with fact that this is not preserved in geom when loaded from pickle on disk\n",
    "        self.glac_geom_srs_wkt = self.glac_geom.GetSpatialReference().ExportToWkt()\n",
    "\n",
    "        #Attributes written by mb_calc\n",
    "        self.z1 = None\n",
    "        self.z1_hs = None\n",
    "        self.z1_stats = None\n",
    "        self.z1_ela = None\n",
    "        self.z2 = None\n",
    "        self.z2_hs = None\n",
    "        self.z2_stats = None\n",
    "        self.z2_ela = None\n",
    "        self.z2_aspect = None\n",
    "        self.z2_aspect_stats = None\n",
    "        self.z2_slope = None\n",
    "        self.z2_slope_stats = None\n",
    "        self.res = None\n",
    "        self.dhdt = None\n",
    "        self.mb = None\n",
    "        self.mb_mean = None\n",
    "        self.t1 = None\n",
    "        self.t2 = None\n",
    "        self.dt = None\n",
    "        self.t1_mean = None\n",
    "        self.t2_mean = None\n",
    "        self.dt_mean = None\n",
    "\n",
    "        self.H = None\n",
    "        self.H_mean = np.nan\n",
    "        self.vx = None\n",
    "        self.vy = None\n",
    "        self.vm = None\n",
    "        self.vm_mean = np.nan\n",
    "        self.divQ = None\n",
    "        self.debris_class = None\n",
    "        self.debris_thick = None\n",
    "        self.debris_thick_mean = np.nan\n",
    "        self.perc_clean = np.nan\n",
    "        self.perc_debris = np.nan\n",
    "        self.perc_pond = np.nan\n",
    "\n",
    "    def geom_srs_update(self, srs=None):\n",
    "        if self.glac_geom.GetSpatialReference() is None:\n",
    "            if srs is None:\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromWkt(self.glac_geom_srs_wkt)\n",
    "            self.glac_geom.AssignSpatialReference(srs)\n",
    "\n",
    "    def geom_attributes(self, srs=None):\n",
    "        self.geom_srs_update()\n",
    "        if srs is not None:\n",
    "            #Should reproject here to equal area, before geom_attributes\n",
    "            #self.glac_geom.AssignSpatialReference(glac_shp_srs)\n",
    "            #self.glac_geom_local = geolib.geom2localortho(self.glac_geom)\n",
    "            geolib.geom_transform(self.glac_geom, srs)\n",
    "\n",
    "        self.glac_geom_extent = geolib.geom_extent(self.glac_geom)\n",
    "        self.glac_area = self.glac_geom.GetArea()\n",
    "        self.glac_area_km2 = self.glac_area / 1E6\n",
    "        self.cx, self.cy = self.glac_geom.Centroid().GetPoint_2D()\n",
    "\n",
    "def srtm_corr(z):\n",
    "    #Should separate into different regions from Kaab et al (2012)\n",
    "    #Should separate into firn/snow, clean ice, and debris-covered ice\n",
    "\n",
    "    #For now, use Kaab et al (2012) region-wide mean of 2.1 +/- 0.4\n",
    "    offset = 2.1\n",
    "    return z + offset\n",
    "\n",
    "def z_vs_dz(z,dz):\n",
    "    plt.scatter(z.compressed(), dz.compressed())\n",
    "\n",
    "#RGI uses 50 m bins\n",
    "def hist_plot(gf, outdir, bin_width=50.0, dz_clim=(-2.0, 2.0)):\n",
    "    #print(\"Generating histograms\")\n",
    "    #Create bins for full range of input data and specified bin width\n",
    "\n",
    "    #NOTE: these counts/areas are for valid pixels only\n",
    "    #Not necessarily a true representation of actual glacier hypsometry\n",
    "    #Need a void-filled DEM for this\n",
    "\n",
    "    z_bin_edges, z_bin_centers = malib.get_bins(gf.z1, bin_width)\n",
    "    #Need to compress here, otherwise histogram uses masked values!\n",
    "    z1_bin_counts, z1_bin_edges = np.histogram(gf.z1.compressed(), bins=z_bin_edges)\n",
    "    z1_bin_areas = z1_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "    #RGI standard is integer thousandths of glaciers total area\n",
    "    #Should check to make sure sum of bin areas equals total area\n",
    "    #z1_bin_areas_perc = 100. * z1_bin_areas / np.sum(z1_bin_areas)\n",
    "    z1_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    #If we only have one elevation grid with dhdt\n",
    "    if gf.z2 is not None:\n",
    "        z2_bin_counts, z2_bin_edges = np.histogram(gf.z2.compressed(), bins=z_bin_edges)\n",
    "        z2_bin_areas = z2_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "        #z2_bin_areas_perc = 100. * z2_bin_areas / np.sum(z2_bin_areas)\n",
    "        z2_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "    else:\n",
    "        z2_bin_counts = z1_bin_counts\n",
    "        z2_bin_edges = z1_bin_edges\n",
    "        z2_bin_areas = z1_bin_areas\n",
    "        z2_bin_areas_perc = z1_bin_areas_perc\n",
    "\n",
    "    #Create arrays to store output\n",
    "    if gf.dhdt is not None:\n",
    "        mb_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        np.ma.set_fill_value(mb_bin_med, np.nan)\n",
    "        mb_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_med = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_count = np.ma.masked_all_like(mb_bin_med)\n",
    "    if gf.vm is not None:\n",
    "        vm_bin_med = np.ma.masked_all_like(mb_bin_med)\n",
    "        vm_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "    if gf.H is not None:\n",
    "        H_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        H_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "    if gf.debris_class is not None:\n",
    "        perc_clean = np.ma.masked_all_like(mb_bin_med)\n",
    "        perc_debris = np.ma.masked_all_like(mb_bin_med)\n",
    "        perc_pond = np.ma.masked_all_like(mb_bin_med)\n",
    "        debris_thick_med = np.ma.masked_all_like(mb_bin_med)\n",
    "        debris_thick_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_clean_bin_med = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_debris_bin_med = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_pond_bin_med = np.ma.masked_all_like(mb_bin_med)\n",
    "\n",
    "        gf.dhdt_clean = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 1).data))\n",
    "        gf.dhdt_debris = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 2).data))\n",
    "        gf.dhdt_pond = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 3).data))\n",
    "\n",
    "    #Bin sample count must be greater than this value\n",
    "    min_bin_samp_count = 9\n",
    "\n",
    "    #Loop through each bin and extract stats\n",
    "    idx = np.digitize(gf.z1, z_bin_edges)\n",
    "    for bin_n in range(z_bin_centers.size):\n",
    "        if gf.dhdt is not None:\n",
    "            mb_bin_samp = gf.mb_map[(idx == bin_n+1)]\n",
    "            if mb_bin_samp.count() > min_bin_samp_count:\n",
    "                mb_bin_med[bin_n] = malib.fast_median(mb_bin_samp)\n",
    "                mb_bin_mad[bin_n] = malib.mad(mb_bin_samp)\n",
    "                mb_bin_mean[bin_n] = mb_bin_samp.mean()\n",
    "                mb_bin_std[bin_n] = mb_bin_samp.std()\n",
    "            dhdt_bin_samp = gf.dhdt[(idx == bin_n+1)]\n",
    "            if dhdt_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_bin_med[bin_n] = malib.fast_median(dhdt_bin_samp)\n",
    "                dhdt_bin_mad[bin_n] = malib.mad(dhdt_bin_samp)\n",
    "                dhdt_bin_mean[bin_n] = dhdt_bin_samp.mean()\n",
    "                dhdt_bin_std[bin_n] = dhdt_bin_samp.std()\n",
    "                dhdt_bin_count[bin_n] = dhdt_bin_samp.count()\n",
    "        if gf.debris_thick is not None:\n",
    "            debris_thick_bin_samp = gf.debris_thick[(idx == bin_n+1)]\n",
    "            if debris_thick_bin_samp.size > min_bin_samp_count:\n",
    "                debris_thick_med[bin_n] = malib.fast_median(debris_thick_bin_samp)\n",
    "                debris_thick_mad[bin_n] = malib.mad(debris_thick_bin_samp)\n",
    "        if gf.debris_class is not None:\n",
    "            debris_class_bin_samp = gf.debris_class[(idx == bin_n+1)]\n",
    "            dhdt_clean_bin_samp = gf.dhdt_clean[(idx == bin_n+1)]\n",
    "            dhdt_debris_bin_samp = gf.dhdt_debris[(idx == bin_n+1)]\n",
    "            dhdt_pond_bin_samp = gf.dhdt_pond[(idx == bin_n+1)]\n",
    "            if debris_class_bin_samp.count() > min_bin_samp_count:\n",
    "                perc_clean[bin_n] = 100. * (debris_class_bin_samp == 1).sum()/debris_class_bin_samp.count()\n",
    "                perc_debris[bin_n] = 100. * (debris_class_bin_samp == 2).sum()/debris_class_bin_samp.count()\n",
    "                perc_pond[bin_n] = 100. * (debris_class_bin_samp == 3).sum()/debris_class_bin_samp.count()\n",
    "            if dhdt_clean_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_clean_bin_med[bin_n] = malib.fast_median(dhdt_clean_bin_samp)\n",
    "            if dhdt_debris_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_debris_bin_med[bin_n] = malib.fast_median(dhdt_debris_bin_samp)\n",
    "            if dhdt_pond_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_pond_bin_med[bin_n] = malib.fast_median(dhdt_pond_bin_samp)\n",
    "        if gf.vm is not None:\n",
    "            vm_bin_samp = gf.vm[(idx == bin_n+1)]\n",
    "            if vm_bin_samp.size > min_bin_samp_count:\n",
    "                vm_bin_med[bin_n] = malib.fast_median(vm_bin_samp)\n",
    "                vm_bin_mad[bin_n] = malib.mad(vm_bin_samp)\n",
    "        if gf.H is not None:\n",
    "            H_bin_samp = gf.H[(idx == bin_n+1)]\n",
    "            if H_bin_samp.size > min_bin_samp_count:\n",
    "                H_bin_mean[bin_n] = H_bin_samp.mean()\n",
    "                H_bin_std[bin_n] = H_bin_samp.std()\n",
    "\n",
    "    dhdt_bin_areas = dhdt_bin_count * gf.res[0] * gf.res[1] / 1E6\n",
    "    #dhdt_bin_areas_perc = 100. * dhdt_bin_areas / np.sum(dhdt_bin_areas)\n",
    "    dhdt_bin_areas_perc = 100. * (dhdt_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    outbins_header = 'bin_center_elev_m, z1_bin_count_valid, z1_bin_area_valid_km2, z1_bin_area_perc, z2_bin_count_valid, z2_bin_area_valid_km2, z2_bin_area_perc'\n",
    "    fmt = '%0.1f, %0.0f, %0.3f, %0.2f, %0.0f, %0.3f, %0.2f'\n",
    "    outbins = [z_bin_centers, z1_bin_counts, z1_bin_areas, z1_bin_areas_perc, z2_bin_counts, z2_bin_areas, z2_bin_areas_perc]\n",
    "    if gf.dhdt is not None:\n",
    "        outbins_header += ', dhdt_bin_count, dhdt_bin_area_valid_km2, dhdt_bin_area_perc, dhdt_bin_med_ma, dhdt_bin_mad_ma, dhdt_bin_mean_ma, dhdt_bin_std_ma, mb_bin_med_mwea, mb_bin_mad_mwea, mb_bin_mean_mwea, mb_bin_std_mwea'\n",
    "        fmt += ', %0.0f, %0.3f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([dhdt_bin_count, dhdt_bin_areas, dhdt_bin_areas_perc, dhdt_bin_med, dhdt_bin_mad, dhdt_bin_mean, dhdt_bin_std, \\\n",
    "                        mb_bin_med, mb_bin_mad, mb_bin_mean, mb_bin_std])\n",
    "    if gf.debris_thick is not None:\n",
    "        outbins_header += ', debris_thick_med_m, debris_thick_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        debris_thick_med[debris_thick_med == -(np.inf)] = 0.00\n",
    "        debris_thick_mad[debris_thick_mad == -(np.inf)] = 0.00\n",
    "        outbins.extend([debris_thick_med, debris_thick_mad])\n",
    "    if gf.debris_class is not None:\n",
    "        outbins_header += ', perc_debris, perc_pond, perc_clean, dhdt_debris_med, dhdt_pond_med, dhdt_clean_med'\n",
    "        fmt += ', %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([perc_debris, perc_pond, perc_clean, dhdt_debris_bin_med, dhdt_pond_bin_med, dhdt_clean_bin_med])\n",
    "    if gf.vm is not None:\n",
    "        outbins_header += ', vm_med, vm_mad'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([vm_bin_med, vm_bin_mad])\n",
    "    if gf.H is not None:\n",
    "        outbins_header += ', H_mean, H_std'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([H_bin_mean, H_bin_std])\n",
    "\n",
    "    outbins = np.ma.array(outbins).T.astype('float32')\n",
    "    np.ma.set_fill_value(outbins, np.nan)\n",
    "    outbins = outbins.filled(np.nan)\n",
    "    outbins_fn = os.path.join(outdir, gf.feat_fn+'_mb_bins.csv')\n",
    "    np.savetxt(outbins_fn, outbins, fmt=fmt, delimiter=',', header=outbins_header)\n",
    "\n",
    "    #Create plots of elevation bins\n",
    "    #print(\"Generating aed plot\")\n",
    "    #f,axa = plt.subplots(1,2, figsize=(6, 6))\n",
    "    nsubplots = 0\n",
    "    if gf.dhdt is not None:\n",
    "        nsubplots += 1\n",
    "    if gf.debris_thick is not None:\n",
    "        nsubplot += 1\n",
    "    if gf.vm is not None:\n",
    "        nsubplot += 1\n",
    "    f,axa = plt.subplots(1,nsubplots, figsize=(10, 7.5))\n",
    "    f.suptitle(gf.feat_fn)\n",
    "    fs = 9\n",
    "    nplot = -1\n",
    "    if gf.dhdt is not None:\n",
    "        nplot += 1\n",
    "        axa[nplot].plot(z1_bin_areas, z_bin_centers, label='%0.2f' % gf.t1_mean)\n",
    "        axa[nplot].axhline(gf.z1_ela, ls=':', c='C0')\n",
    "        if gf.z2 is not None:\n",
    "            axa[nplot].plot(z2_bin_areas, z_bin_centers, label='%0.2f' % gf.t2_mean)\n",
    "            axa[nplot].axhline(gf.z2_ela, ls=':', c='C1')\n",
    "        axa[nplot].legend(prop={'size':8}, loc='upper right')\n",
    "        axa[nplot].set_ylabel('Elevation (m WGS84)', fontsize=fs)\n",
    "        axa[nplot].set_xlabel('Area $\\mathregular{km^2}$', fontsize=fs)\n",
    "        axa[nplot].yaxis.set_ticks_position('both')\n",
    "        # pltlib.minorticks_on(axa[0])\n",
    "\n",
    "        nplot += 1\n",
    "        axa[nplot].axvline(0, lw=1.0, c='k')\n",
    "        \"\"\"\n",
    "        #Plot flux divergence values for each bin\n",
    "        if gf.vm is not None and gf.H is not None:\n",
    "            divQ_bin_mean = np.gradient(H_bin_mean * vm_bin_med * v_col_f)\n",
    "            axa[1].plot(divQ_bin_mean, z_bin_centers, color='green')\n",
    "        \"\"\"\n",
    "        axa[nplot].plot(mb_bin_med, z_bin_centers, color='k')\n",
    "        axa[nplot].axvline(gf.mb_mean, lw=0.5, ls=':', c='k', label='%0.2f m w.e./yr' % gf.mb_mean)\n",
    "        axa[nplot].fill_betweenx(z_bin_centers, mb_bin_med-mb_bin_mad, mb_bin_med+mb_bin_mad, color='k', alpha=0.1)\n",
    "        axa[nplot].fill_betweenx(z_bin_centers, 0, mb_bin_med, where=(mb_bin_med<0), color='r', alpha=0.2)\n",
    "        axa[nplot].fill_betweenx(z_bin_centers, 0, mb_bin_med, where=(mb_bin_med>0), color='b', alpha=0.2)\n",
    "        #axa[nplot].set_xlabel('dh/dt (m/yr)')\n",
    "        axa[nplot].set_xlabel('Mass balance (m w.e./yr)', fontsize=fs)\n",
    "        axa[nplot].legend(prop={'size':8}, loc='upper right')\n",
    "        axa[nplot].yaxis.set_ticks_position('both')\n",
    "        # pltlib.minorticks_on(axa[1])\n",
    "        #Hide y-axis labels\n",
    "        axa[nplot].axes.yaxis.set_ticklabels([])\n",
    "        axa[nplot].set_xlim(*dz_clim)\n",
    "\n",
    "    if gf.debris_thick is not None:\n",
    "        nplot += 1\n",
    "        axa[nplot].errorbar(debris_thick_med*100., z_bin_centers, xerr=debris_thick_mad*100, color='k', fmt='o', ms=3, label='Debris Thickness', alpha=0.6)\n",
    "    if gf.debris_class is not None:\n",
    "        axa[nplot].plot(perc_debris, z_bin_centers, color='sienna', label='Debris Coverage')\n",
    "        axa[nplot].plot(perc_pond, z_bin_centers, color='turquoise', label='Pond Coverage')\n",
    "    if gf.debris_thick is not None or gf.debris_class is not None:\n",
    "        axa[nplot].set_xlim(0, 100)\n",
    "        axa[nplot].yaxis.set_ticks_position('both')\n",
    "        # pltlib.minorticks_on(axa[2])\n",
    "        axa[nplot].axes.yaxis.set_ticklabels([])\n",
    "        axa[nplot].legend(prop={'size':8}, loc='upper right')\n",
    "        axa[nplot].set_xlabel('Debris thickness (cm), coverage (%)', fontsize=fs)\n",
    "\n",
    "    if gf.vm is not None:\n",
    "        nplot += 1\n",
    "        ax4 = axa[nplot].twinx()\n",
    "        ax4.set_xlabel('Velocity (m/yr)', fontsize=fs)\n",
    "        ax4.plot(vm_bin_med, z_bin_centers, color='g', label='Vm (%0.2f m/yr)' % gf.vm_mean)\n",
    "        ax4.fill_betweenx(z_bin_centers, vm_bin_med-vm_bin_mad, vm_bin_med+vm_bin_mad, color='g', alpha=0.1)\n",
    "        #ax4.set_xlim(0, 50)\n",
    "        ax4.xaxis.tick_top()\n",
    "        ax4.xaxis.set_label_position(\"top\")\n",
    "        ax4.legend(prop={'size':8}, loc='upper right')\n",
    "\n",
    "    if gf.H is not None:\n",
    "        axa[nplot].plot(H_bin_mean, z_bin_centers, color='b', label='H (%0.2f m)' % gf.H_mean)\n",
    "        axa[nplot].fill_betweenx(z_bin_centers, H_bin_mean-H_bin_std, H_bin_mean+H_bin_std, color='b', alpha=0.1)\n",
    "        axa[nplot].set_xlabel('Ice Thickness (m)', fontsize=fs)\n",
    "        axa[nplot].legend(prop={'size':8}, loc='lower right')\n",
    "        # pltlib.minorticks_on(axa[3])\n",
    "        #axa[nplot].set_xlim(0, 400)\n",
    "        axa[nplot].yaxis.tick_right()\n",
    "        axa[nplot].yaxis.set_ticks_position('both')\n",
    "        axa[nplot].yaxis.set_label_position(\"right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #Make room for suptitle\n",
    "    plt.subplots_adjust(top=0.95, wspace=0.1)\n",
    "    #print(\"Saving aed plot\")\n",
    "    fig_fn = os.path.join(outdir, gf.feat_fn+'_mb_aed.png')\n",
    "    #plt.savefig(fig_fn, bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(fig_fn, dpi=300)\n",
    "    plt.close(f)\n",
    "    return z_bin_edges\n",
    "\n",
    "def map_plot(gf, z_bin_edges, outdir, hs=True, dz_clim=(-2.0, 2.0)):\n",
    "    #print(\"Generating map plot\")\n",
    "    f,axa = plt.subplots(1,3, figsize=(10,7.5))\n",
    "    #f.suptitle(gf.feat_fn)\n",
    "    alpha = 1.0\n",
    "    if hs:\n",
    "        #z1_hs = geolib.gdaldem_wrapper(gf.out_z1_fn, product='hs', returnma=True, verbose=False)\n",
    "        #z2_hs = geolib.gdaldem_wrapper(gf.out_z2_fn, product='hs', returnma=True, verbose=False)\n",
    "        z1_hs = gf.z1_hs\n",
    "        z2_hs = gf.z2_hs\n",
    "        hs_clim = malib.calcperc(z2_hs, (2,98))\n",
    "        z1_hs_im = axa[0].imshow(z1_hs, cmap='gray', vmin=hs_clim[0], vmax=hs_clim[1])\n",
    "        z2_hs_im = axa[1].imshow(z2_hs, cmap='gray', vmin=hs_clim[0], vmax=hs_clim[1])\n",
    "        alpha = 0.5\n",
    "    z1_im = axa[0].imshow(gf.z1, cmap='cpt_rainbow', vmin=z_bin_edges[0], vmax=z_bin_edges[-1], alpha=alpha)\n",
    "    z2_im = axa[1].imshow(gf.z2, cmap='cpt_rainbow', vmin=z_bin_edges[0], vmax=z_bin_edges[-1], alpha=alpha)\n",
    "    axa[0].contour(gf.z1, [gf.z1_ela,], linewidths=0.5, linestyles=':', colors='w')\n",
    "    axa[1].contour(gf.z2, [gf.z2_ela,], linewidths=0.5, linestyles=':', colors='w')\n",
    "    #t1_title = int(np.round(gf.t1))\n",
    "    #t2_title = int(np.round(gf.t2))\n",
    "    t1_title = '%0.2f' % gf.t1_mean\n",
    "    t2_title = '%0.2f' % gf.t2_mean\n",
    "    #t1_title = gf.t1.strftime('%Y-%m-%d')\n",
    "    #t2_title = gf.t2.strftime('%Y-%m-%d')\n",
    "    axa[0].set_title(t1_title)\n",
    "    axa[1].set_title(t2_title)\n",
    "    axa[2].set_title('%s to %s (%0.2f yr)' % (t1_title, t2_title, gf.dt_mean))\n",
    "    dz_im = axa[2].imshow(gf.dhdt, cmap='RdBu', vmin=dz_clim[0], vmax=dz_clim[1])\n",
    "    for ax in axa:\n",
    "        # pltlib.hide_ticks(ax)\n",
    "        ax.set_facecolor('k')\n",
    "    # sb_loc = pltlib.best_scalebar_location(gf.z1)\n",
    "    # pltlib.add_scalebar(axa[0], gf.res[0], location=sb_loc)\n",
    "    # pltlib.add_cbar(axa[0], z1_im, label='Elevation (m WGS84)')\n",
    "    # pltlib.add_cbar(axa[1], z2_im, label='Elevation (m WGS84)')\n",
    "    # pltlib.add_cbar(axa[2], dz_im, label='dh/dt (m/yr)')\n",
    "    plt.tight_layout()\n",
    "    #Make room for suptitle\n",
    "    #plt.subplots_adjust(top=0.90)\n",
    "    #print(\"Saving map plot\")\n",
    "    fig_fn = os.path.join(outdir, gf.feat_fn+'_mb_map.png')\n",
    "    plt.savefig(fig_fn, dpi=300)\n",
    "    plt.close(f)\n",
    "\n",
    "def get_date_a(ds, date_shp_lyr, glac_geom_mask, datefield):\n",
    "    date_r_ds = iolib.mem_drv.CreateCopy('', ds)\n",
    "    #Shapefile order should be sorted by time, but might want to think about sorting here\n",
    "    #Can automatically search for datefield\n",
    "    gdal.RasterizeLayer(date_r_ds, [1], date_shp_lyr, options=[\"ATTRIBUTE=%s\" % datefield])\n",
    "    date_a = np.ma.array(iolib.ds_getma(date_r_ds), mask=glac_geom_mask)\n",
    "    #Note: NED dates are in integer years, assume source imagery was flown in late summer for mountains\n",
    "    if datefield == 'S_DATE_CLN':\n",
    "        date_a += 0.75\n",
    "    return date_a\n",
    "\n",
    "\"\"\"\n",
    "#Consider storing setup variables in dictionary that can be passed to Process\n",
    "setup = {}\n",
    "setup['site'] = site\n",
    "\"\"\"\n",
    "\n",
    "topdir='/Users/davidrounce/Documents/Dave_Rounce/HiMAT/DEMs/'\n",
    "site = 'Braun_PCR07N'\n",
    "# Braun options: ['Braun_PCR07N', 'Braun_PCR08N', 'Braun_PCR09N']\n",
    "# site='StElias'\n",
    "# site = 'Chugach'\n",
    "# site = 'AR_W'\n",
    "# site = 'AR_C'\n",
    "# site = 'AR_E'\n",
    "# site = 'Coast'\n",
    "# site = 'Kenai'\n",
    "# site = 'AK_Pen'\n",
    "# Berthier options: ['StElias', 'Chugach', 'AR_W', 'AR_C', 'AR_E', 'Coast', 'Kenai', 'AK_Pen', 'HMA']\n",
    "\n",
    "#Filter glacier poly - let's stick with big glaciers for now\n",
    "# min_glac_area = 0.0 #km^2\n",
    "#min_glac_area = 0.1 #km^2\n",
    "min_glac_area = 0 #km^2\n",
    "# min_glac_area = 2. #km^2\n",
    "#Only write out for larger glaciers\n",
    "min_glac_area_writeout = 1.\n",
    "#Minimum percentage of glacier poly covered by valid dz\n",
    "min_valid_area_perc = 0.6       # DSHEAN WAS 0.85\n",
    "#Process thickness, velocity, etc\n",
    "extra_layers = False            # DSHEAN WAS TRUE\n",
    "#Write out DEMs and dz map\n",
    "writeout = True\n",
    "#Generate figures\n",
    "mb_plot = True\n",
    "#Run in parallel, set to False for serial loop\n",
    "parallel = False\n",
    "#Verbose for debugging\n",
    "verbose = True\n",
    "#Number of parallel processes\n",
    "#Use all virtual cores\n",
    "#nproc = iolib.cpu_count(logical=True) - 1\n",
    "#Use all physical cores\n",
    "# nproc = iolib.cpu_count(logical=False) - 1\n",
    "nproc = 1\n",
    "#Shortcut to use existing glacfeat_list.p if found\n",
    "use_existing_glacfeat = True\n",
    "\n",
    "#Pad by this distance (meters) around glacier polygon for uncertainty estimates over surrounding surfaces\n",
    "buff_dist = 1000\n",
    "\n",
    "#Bin width\n",
    "bin_width = 50\n",
    "\n",
    "#Surface to column average velocity scaling\n",
    "v_col_f = 0.8\n",
    "\n",
    "#This is recommendation by Huss et al (2013)\n",
    "rho_is = 0.85\n",
    "rho_sigma = 0.06\n",
    "\n",
    "#If breaking down into accumulation vs. ablation area\n",
    "#rho_i = 0.91\n",
    "#rho_s = 0.50\n",
    "#rho_f = 0.60\n",
    "\n",
    "#Fountain\n",
    "#Other sources Kaab et al (2012) use 0.1\n",
    "area_sigma_perc = 0.09\n",
    "\n",
    "global z1_date\n",
    "global z2_date\n",
    "z1_date = None\n",
    "z2_date = None\n",
    "z1_srtm_penetration_corr = False\n",
    "z2_srtm_penetration_corr = False\n",
    "\n",
    "\n",
    "if site in ['Braun_PCR07N', 'Braun_PCR08N', 'Braun_PCR09N']:\n",
    "    #Output directory\n",
    "    outdir = topdir + 'Braun/output/'\n",
    "    outdir_fig = topdir + 'Braun/output/figures/'\n",
    "    outdir_csv = topdir + 'Braun/output/csv/'\n",
    "\n",
    "    glac_shp_fn = topdir + '../RGI/rgi60/01_rgi60_Alaska/01_rgi60_Alaska.shp'\n",
    "    glacfeat_fn = outdir + site + '_glacfeat_list.p'\n",
    "\n",
    "    # Filenames\n",
    "    z1_fn_dict = {'Braun_PCR07N': topdir + 'Braun/TDX-SRTM_prelim/NorthAmerica_dhdt/region_03_PCRn/srtm_filled_ice__03_PCRn-utm07N-strips_crp2reg_03_PCRn.tif',\n",
    "                  'Braun_PCR08N': topdir + 'Braun/TDX-SRTM_prelim/NorthAmerica_dhdt/region_03_PCRn/srtm_filled_ice__03_PCRn-utm08N-strips_crp2reg_03_PCRn.tif',\n",
    "                  'Braun_PCR09N': topdir + 'Braun/TDX-SRTM_prelim/NorthAmerica_dhdt/region_03_PCRn/srtm_filled_ice__03_PCRn-utm09N-strips_crp2reg_03_PCRn.tif'}\n",
    "    z1_date_dict = 2000.128\n",
    "    z2_fn_dict = None\n",
    "    z2_date_dict = 2012.0\n",
    "    dhdt_fn_dict = {'Braun_PCR07N': topdir + 'Braun/TDX-SRTM_prelim/NorthAmerica_dhdt/region_03_PCRn/dh_dt_on_ice__03_PCRn-utm07N-strips_crp2reg_03_PCRn.tif',\n",
    "                    'Braun_PCR08N': topdir + 'Braun/TDX-SRTM_prelim/NorthAmerica_dhdt/region_03_PCRn/dh_dt_on_ice__03_PCRn-utm08N-strips_crp2reg_03_PCRn.tif',\n",
    "                    'Braun_PCR09N': topdir + 'Braun/TDX-SRTM_prelim/NorthAmerica_dhdt/region_03_PCRn/dh_dt_on_ice__03_PCRn-utm09N-strips_crp2reg_03_PCRn.tif'}\n",
    "    \n",
    "    \n",
    "    z1_fn = z1_fn_dict[site]\n",
    "    z1_date = z1_date_dict\n",
    "    z1_sigma = 10\n",
    "    z1_srtm_penetration_corr = False\n",
    "\n",
    "    if z2_fn_dict is None:\n",
    "        dhdt_fn = dhdt_fn_dict[site]\n",
    "\n",
    "        # Hack - use z1 and dhdt to produce z2, so Shean processing scripts can be used for MB and binning calcs with Braun data\n",
    "        #Output projection\n",
    "        #'+proj=aea +lat_1=25 +lat_2=47 +lat_0=36 +lon_0=85 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs '\n",
    "        ds = gdal.Open(z1_fn)\n",
    "        prj = ds.GetProjection()\n",
    "        srs = osr.SpatialReference(wkt=prj)\n",
    "        aea_srs = srs\n",
    "        \n",
    "        #Warp everything to common res/extent/proj\n",
    "        ds_list = warplib.memwarp_multi_fn([z1_fn, dhdt_fn], \n",
    "                                           res='min', t_srs=aea_srs, verbose=verbose, r='cubic')\n",
    "        # DEM masks\n",
    "        ds_list_masked = [iolib.ds_getma(i) for i in ds_list]\n",
    "        z1 = np.ma.masked_less_equal(ds_list_masked[0], 0)\n",
    "        dhdt = ds_list_masked[1]\n",
    "        \n",
    "        # Create z2 from z1 and dhdt\n",
    "        z2 = z1 + dhdt * (z2_date_dict - z1_date_dict)\n",
    "        z2.mask = np.ma.mask_or(z1.mask, dhdt.mask)\n",
    "\n",
    "        # Write out file\n",
    "        z2_fn = z1_fn.replace('srtm_filled_ice', 'z2_fromSTRM&dhdt')\n",
    "        iolib.writeGTiff(z2, z2_fn, src_ds=ds_list[0]) \n",
    "        \n",
    "    else:\n",
    "        z2_fn = z2_fn_dict[site]\n",
    "    z2_date = z2_date_dict\n",
    "    z2_sigma = 10\n",
    "    z2_srtm_penetration_corr = False\n",
    "    \n",
    "\n",
    "    #Output projection\n",
    "    #'+proj=aea +lat_1=25 +lat_2=47 +lat_0=36 +lon_0=85 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs '\n",
    "    # print('\\n\\nSHOULD CHANGE TO EQUAL AREA PROJECTION!\\n\\n')\n",
    "    # aea_srs = geolib.hma_aea_srs\n",
    "    ds = gdal.Open(z1_fn)\n",
    "    prj = ds.GetProjection()\n",
    "    srs = osr.SpatialReference(wkt=prj)\n",
    "    aea_srs = srs\n",
    "\n",
    "    #Surface velocity\n",
    "    # add surface velocities where possible?\n",
    "\n",
    "    print('\\nStatic analysis does not work for quantifying uncertainty because clipped by RGI extents')\n",
    "    print('\\nOpting to use UTM projections to avoid errors caused by projecting/resampling datasets\\n')\n",
    "    \n",
    "elif site in ['StElias', 'Chugach', 'AR_W', 'AR_C', 'AR_E', 'Coast', 'Kenai', 'AK_Pen']:\n",
    "    #Output directory\n",
    "    outdir = topdir + 'Berthier/output/'\n",
    "    outdir_fig = topdir + 'Berthier/output/figures/'\n",
    "    outdir_csv = topdir + 'Berthier/output/csv'\n",
    "\n",
    "    glac_shp_fn = topdir + '../RGI/rgi60/01_rgi60_Alaska/01_rgi60_Alaska.shp'\n",
    "    glacfeat_fn = outdir + site + '_glacfeat_list.p'\n",
    "\n",
    "    #ASTER+WV trend interp 2008\n",
    "    z1_fn_dict = {'StElias': topdir + 'Berthier/Alaska_1950_2006/1.StElias/StElias_Map_DEM.tif',\n",
    "                  'Chugach': topdir + 'Berthier/Alaska_1950_2006/2.Chugach/Chugach_Map_DEM.tif',\n",
    "                  'AR_W': topdir + 'Berthier/Alaska_1950_2006/3.AR_W/AR_W_Map_DEM.tif',\n",
    "                  'AR_C': topdir + 'Berthier/Alaska_1950_2006/4.AR_C/AR_C_Map_DEM.tif',\n",
    "                  'AR_E': topdir + 'Berthier/Alaska_1950_2006/5.AR_E/AR_E_Map_DEM.tif',\n",
    "                  'Coast': topdir + 'Berthier/Alaska_1950_2006/6.Coast/Coast_Map_DEM.tif',\n",
    "                  'Kenai': topdir + 'Berthier/Alaska_1950_2006/7.Kenai/Kenai_Map_DEM.tif',\n",
    "                  'AK_Pen': topdir + 'Berthier/Alaska_1950_2006/8.AK_Peninsula/AK_Peninsula_Map_DEM.tif',}\n",
    "    z1_date_dict = {'StElias': 1968.,\n",
    "                    'Chugach': 1954.,\n",
    "                    'AR_W': 1953.,\n",
    "                    'AR_C': 1953.,\n",
    "                    'AR_E': 1953.,\n",
    "                    'Coast': 1966.,\n",
    "                    'Kenai': 1950.,\n",
    "                    'AK_Pen': 1950.}\n",
    "    z2_fn_dict = {'StElias': topdir + 'Berthier/Alaska_1950_2006/1.StElias/StElias_Satellite_DEM.tif',\n",
    "                  'Chugach': topdir + 'Berthier/Alaska_1950_2006/2.Chugach/Chugach_Sat_DEM.tif',\n",
    "                  'AR_W': topdir + 'Berthier/Alaska_1950_2006/3.AR_W/AR_W_Sat_DEM.tif',\n",
    "                  'AR_C': topdir + 'Berthier/Alaska_1950_2006/4.AR_C/AR_C_Sat_DEM.tif',\n",
    "                  'AR_E': topdir + 'Berthier/Alaska_1950_2006/5.AR_E/AR_E_Sat_DEM.tif',\n",
    "                  'Coast': topdir + 'Berthier/Alaska_1950_2006/6.Coast/Coast_Sat_DEM.tif',\n",
    "                  'Kenai': topdir + 'Berthier/Alaska_1950_2006/7.Kenai/Kenai_Sat_DEM.tif',\n",
    "                  'AK_Pen': topdir + 'Berthier/Alaska_1950_2006/8.AK_Peninsula/AK_Peninsula_Sat_DEM.tif',}\n",
    "    z2_date_dict = {'StElias': 2006.75,\n",
    "                    'Chugach': 2006.75,\n",
    "                    'AR_W': 2004.75,\n",
    "                    'AR_C': 2004.75,\n",
    "                    'AR_E': 2004.75,\n",
    "                    'Coast': 2007.75,\n",
    "                    'Kenai': 2007.75,\n",
    "                    'AK_Pen': 2007.75}\n",
    "\n",
    "    z1_fn = z1_fn_dict[site]\n",
    "    z1_date = z1_date_dict[site]\n",
    "    z1_sigma = 10\n",
    "    z1_srtm_penetration_corr = False\n",
    "\n",
    "    #WV trend interp 2018\n",
    "    # z2_fn = topdir + 'Berthier/Alaska_1950_2006/1.StElias/StElias_Satellite_DEM.tif'\n",
    "    z2_fn = z2_fn_dict[site]\n",
    "    z2_date = z2_date_dict[site]\n",
    "    z2_sigma = 10\n",
    "    z2_srtm_penetration_corr = False\n",
    "\n",
    "    #Output projection\n",
    "    #'+proj=aea +lat_1=25 +lat_2=47 +lat_0=36 +lon_0=85 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs '\n",
    "    # print('\\n\\nSHOULD CHANGE TO EQUAL AREA PROJECTION!\\n\\n')\n",
    "    # aea_srs = geolib.hma_aea_srs\n",
    "    ds = gdal.Open(z1_fn)\n",
    "    prj = ds.GetProjection()\n",
    "    srs = osr.SpatialReference(wkt=prj)\n",
    "    aea_srs = srs\n",
    "\n",
    "    #Surface velocity\n",
    "    # add surface velocities where possible?\n",
    "\n",
    "    print('\\nStatic analysis does not work for quantifying uncertainty because glacier exceeded RGI extents in 1950s')\n",
    "    print('\\nOpting to use UTM projections to avoid errors caused by projecting/resampling datasets\\n')\n",
    "\n",
    "elif site == 'hma':\n",
    "    glac_shp_fn = os.path.join(topdir,'data/rgi60/regions/rgi60_merge_HMA_aea.shp')\n",
    "    #glac_shp_fn = '/nobackupp8/deshean/hma/aster/dsm/aster_align_index_2000-2018_aea_stack/mb_test/rgi_ngozumpa.shp'\n",
    "    glacfeat_fn = os.path.splitext(glac_shp_fn)[0]+'_glacfeat_list.p'\n",
    "\n",
    "    #ASTER+WV trend interp 2008\n",
    "    z1_fn = '/nobackupp8/deshean/hma/combined_aster_wv/dem_align_ASTER_WV_index_2000-2018_aea_stack/dem_align_ASTER_WV_index_2000-2018_aea_20000531_mos_retile.vrt'\n",
    "    z1_date = 2000.412\n",
    "    z1_sigma = 4.0\n",
    "    z1_srtm_penetration_corr = False\n",
    "\n",
    "    #WV trend interp 2018\n",
    "    z2_fn = '/nobackupp8/deshean/hma/combined_aster_wv/dem_align_ASTER_WV_index_2000-2018_aea_stack/dem_align_ASTER_WV_index_2000-2018_aea_20180531_mos_retile.vrt'\n",
    "    z2_date = 2018.412\n",
    "    z2_sigma = 4.0\n",
    "    z2_srtm_penetration_corr = False\n",
    "\n",
    "    #Output directory\n",
    "    outdir = os.path.join(os.path.split(z2_fn)[0], 'mb_combined_20190206')\n",
    "    outdir_fig = outdir\n",
    "    outdir_csv = outdir\n",
    "    #outdir = '/nobackup/deshean/hma/aster/dsm/aster_align_index_2000-2018_aea_stack/mb'\n",
    "\n",
    "    #Output projection\n",
    "    #'+proj=aea +lat_1=25 +lat_2=47 +lat_0=36 +lon_0=85 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs '\n",
    "    aea_srs = geolib.hma_aea_srs\n",
    "\n",
    "    #Surface velocity\n",
    "    #Note: had to force srs on Amaury's original products\n",
    "    #gdal_edit.py -a_srs '+proj=lcc +lat_1=28 +lat_2=32 +lat_0=90 +lon_0=85 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs' fn\n",
    "    #v_dir = '/nobackup/deshean/rpcdem/hma/velocity_jpl_amaury_2013-2015'\n",
    "    v_dir = '/nobackup/deshean/data/jpl_vel'\n",
    "    vx_fn = os.path.join(v_dir, 'HMA_G0240_0000_vx_masked.tif')\n",
    "    vy_fn = os.path.join(v_dir, 'HMA_G0240_0000_vy_masked.tif')\n",
    "\n",
    "else:\n",
    "    sys.exit(\"Must specify input site\")\n",
    "\n",
    "\n",
    "    \n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "if not os.path.exists(outdir_fig):\n",
    "    os.makedirs(outdir_fig)\n",
    "if not os.path.exists(outdir_csv):\n",
    "    os.makedirs(outdir_csv)\n",
    "\n",
    "ts = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "out_fn = '%s_mb_%s.csv' % (site, ts)\n",
    "out_fn = os.path.join(outdir, out_fn)\n",
    "\n",
    "#List to hold output\n",
    "out = []\n",
    "\n",
    "if 'rgi' in glac_shp_fn:\n",
    "    #Use RGI\n",
    "    glacname_fieldname = \"Name\"\n",
    "    #RGIId (String) = RGI50-01.00004\n",
    "    glacnum_fieldname = \"RGIId\"\n",
    "    glacnum_fmt = '%08.5f'\n",
    "else:\n",
    "    sys.exit('Unrecognized glacier shp filename')\n",
    "\n",
    "#Set up output header\n",
    "#out_header = '%s,x,y,z_med,z_min,z_max,z_p16,z_p84,z_slope,z_aspect,dhdt_ma,dhdt_ma_sigma,mb_mwea,mb_mwea_sigma,area_m2,mb_m3wea,mb_m3wea_sigma,t1,t2,dt,valid_area_perc' % glacnum_fieldname\n",
    "out_header = '%s,x,y,z_med,z_min,z_max,z_slope,z_aspect,dhdt_ma,dhdt_ma_sigma,mb_mwea,mb_mwea_sigma,area_m2,mb_m3wea,mb_m3wea_sigma,t1,t2,dt,valid_area_perc' % glacnum_fieldname\n",
    "if extra_layers:\n",
    "    out_header += ',H_m'\n",
    "    if site == 'hma':\n",
    "        out_header += ',debris_m,perc_debris,perc_pond,perc_clean'\n",
    "        out_header += ',vm_ma'\n",
    "\n",
    "nf = len(out_header.split(','))\n",
    "out_fmt = [glacnum_fmt,] + ['%0.3f'] * (nf - 1)\n",
    "\n",
    "\n",
    "# Shape layer processing\n",
    "glac_shp_init = gpd.read_file(glac_shp_fn)\n",
    "if verbose:\n",
    "    print('Shp init crs:', glac_shp_init.crs)\n",
    "# ax = glac_shp_wgs84.plot()\n",
    "# ax.set_title(\"WGS84 (lat/lon)\"\n",
    "\n",
    "# If projected shapefile already exists, then skip projection\n",
    "glac_shp_proj_fn = (outdir + glac_shp_fn.split('/')[-1].replace('.shp','_crs' +\n",
    "                                                                str(aea_srs.GetAttrValue(\"AUTHORITY\", 1)) + '.shp'))\n",
    "if os.path.exists(glac_shp_proj_fn) == False:\n",
    "    glac_shp_proj = glac_shp_init.to_crs({'init': 'epsg:' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1))})\n",
    "    glac_shp_proj.to_file(glac_shp_proj_fn)\n",
    "\n",
    "glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "#This should be contained in features\n",
    "glac_shp_srs = glac_shp_lyr.GetSpatialRef()\n",
    "feat_count = glac_shp_lyr.GetFeatureCount()\n",
    "print(\"Input glacier polygon count: %i\" % feat_count)\n",
    "\n",
    "z1_ds = gdal.Open(z1_fn)\n",
    "z2_ds = gdal.Open(z2_fn)\n",
    "dz_int_geom = geolib.ds_geom_intersection([z1_ds, z2_ds], t_srs=glac_shp_srs)\n",
    "\n",
    "#Spatial filter\n",
    "glac_shp_lyr.SetSpatialFilter(dz_int_geom)\n",
    "feat_count = glac_shp_lyr.GetFeatureCount()\n",
    "print(\"Glacier polygon count after spatial filter: %i\" % feat_count)\n",
    "glac_shp_lyr.ResetReading()\n",
    "\n",
    "#Area filter\n",
    "glac_shp_lyr.SetAttributeFilter(\"Area > %s\" % min_glac_area)\n",
    "feat_count = glac_shp_lyr.GetFeatureCount()\n",
    "print(\"Min. Area filter glacier polygon count: %i\" % feat_count)\n",
    "glac_shp_lyr.ResetReading()\n",
    "\n",
    "print(\"Processing %i features\" % feat_count)\n",
    "\n",
    "#Set higher stripe count so we're not thrashing one disk\n",
    "#cmd = ['lfs', 'setstripe', '-c', str(nproc), outdir]\n",
    "#subprocess.call(cmd)\n",
    "# iolib.setstripe(outdir, nproc)        # REMOVED THIS BECAUSE IT WAS CAUSING subprocess error - likely for spc?\n",
    "\n",
    "#Create a list of glacfeat objects (contains geom) - safe for multiprocessing, while OGR layer is not\n",
    "if os.path.exists(glacfeat_fn) and use_existing_glacfeat:\n",
    "    print(\"Loading %s\" % glacfeat_fn)\n",
    "    #This fails to load geometry srs\n",
    "    glacfeat_list = pickle.load(open(glacfeat_fn,\"rb\"))\n",
    "else:\n",
    "    glacfeat_list = []\n",
    "    print(\"Generating %s\" % glacfeat_fn)\n",
    "    for n, feat in enumerate(glac_shp_lyr):\n",
    "        gf = GlacFeat(feat, glacname_fieldname, glacnum_fieldname)\n",
    "        print(\"%i of %i: %s\" % (n+1, feat_count, gf.feat_fn))\n",
    "        #Calculate area, extent, centroid\n",
    "        #NOTE: Input must be in projected coordinate system, ideally equal area\n",
    "        #Should check this and reproject\n",
    "        gf.geom_attributes(srs=aea_srs)\n",
    "        glacfeat_list.append(gf)\n",
    "    pickle.dump(glacfeat_list, open(glacfeat_fn,\"wb\"))\n",
    "\n",
    "glac_shp_lyr = None\n",
    "glac_shp_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== TESTING Z2 CREATION WORKS =====\n",
    "# #Warp everything to common res/extent/proj\n",
    "# ds_list = warplib.memwarp_multi_fn([z1_fn, dhdt_fn, z2_fn], \n",
    "#                                    res='min', t_srs=aea_srs, verbose=verbose, r='cubic')\n",
    "# # DEM masks\n",
    "# ds_list_masked = [iolib.ds_getma(i) for i in ds_list]\n",
    "# z1 = np.ma.masked_less_equal(ds_list_masked[0], 0)\n",
    "# dhdt = ds_list_masked[1]\n",
    "# z2 = ds_list_masked[2]\n",
    "\n",
    "# titles = ['z1']\n",
    "# clim = malib.calcperc(z1, (2,98))\n",
    "# plot_array(z1, clim, titles, 'inferno', 'Elevation (m WGS84)', fn='z1.png')\n",
    "\n",
    "# titles = ['dhdt']\n",
    "# clim = malib.calcperc(dhdt, (2,98))\n",
    "# plot_array(dhdt, clim, titles, 'inferno', 'Elevation (m WGS84)', fn='z1.png')\n",
    "\n",
    "# titles = ['z2']\n",
    "# clim = malib.calcperc(z2, (2,98))\n",
    "# plot_array(z2, clim, titles, 'inferno', 'Elevation (m WGS84)', fn='z2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "# glacfeat_list_in = [glacfeat_list[240]]\n",
    "glacfeat_list_in = glacfeat_list[0:2]\n",
    "# glacfeat_list_in = glacfeat_list\n",
    "\n",
    "#This is a hack to limit processing for just a few glaciers\n",
    "glac_dict = None\n",
    "#Ngozumpa, Khumbu etc\n",
    "#glac_dict = ['15.03474', '15.03733', '15.10070', '15.09991']\n",
    "\n",
    "if glac_dict:\n",
    "    glacfeat_list_in = []\n",
    "    for i in glacfeat_list:\n",
    "        if i.glacnum in glac_dict:\n",
    "            glacfeat_list_in.append(i)\n",
    "\n",
    "gf = glacfeat_list_in[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12645_EastYakutatGlacier\n",
      "output_fn: /Users/davidrounce/Documents/Dave_Rounce/HiMAT/DEMs/Braun/output/1.12645_EastYakutatGlacier_mb.csv\n"
     ]
    }
   ],
   "source": [
    "print(gf.feat_fn)\n",
    "\n",
    "out_csv_fn = os.path.join(outdir, gf.feat_fn+'_mb.csv')\n",
    "if verbose:\n",
    "    print('output_fn:', out_csv_fn)\n",
    "if not os.path.exists(out_csv_fn):\n",
    "    #This should already be handled by earlier attribute filter, but RGI area could be wrong\n",
    "    if gf.glac_area_km2 < min_glac_area:\n",
    "        if verbose:\n",
    "            print(\"Glacier area of %0.1f is below %0.1f km2 threshold\" % (gf.glac_area_km2, min_glac_area))\n",
    "#         return None\n",
    "\n",
    "    fn_dict = OrderedDict()\n",
    "    #We at least want to warp the two input DEMs\n",
    "    fn_dict['z1'] = z1_fn\n",
    "    fn_dict['z2'] = z2_fn\n",
    "\n",
    "    if extra_layers and (gf.glac_area_km2 > min_glac_area_writeout):\n",
    "        #Attempt to load Huss ice thickness grid\n",
    "        huss_dir = os.path.join(topdir, 'data/huss')\n",
    "        ice_thick_fn = os.path.join(huss_dir, 'RGI%02i_thick/thickness/thick_%05i.agr' % \\\n",
    "                tuple(map(int, gf.glacnum.split('.'))))\n",
    "        if os.path.exists(ice_thick_fn):\n",
    "            fn_dict['ice_thick'] = ice_thick_fn\n",
    "\n",
    "        if site == 'hma':\n",
    "            #Add debris cover datasets\n",
    "            #Should tar these up, and extract only necessary file\n",
    "            #Downloaded from: http://mountainhydrology.org/data-nature-2017/\n",
    "            kra_nature_dir = '/nobackup/deshean/data/Kraaijenbrink_hma/regions/out'\n",
    "            #This assumes that numbers are identical between RGI50 and RGI60\n",
    "            debris_class_fn = os.path.join(kra_nature_dir, 'RGI50-%s/classification.tif' % gf.glacnum)\n",
    "            debris_thick_fn = os.path.join(kra_nature_dir, 'RGI50-%s/debris-thickness-50cm.tif' % gf.glacnum)\n",
    "            #ice_thick_fn = os.path.join(kra_nature_dir, 'RGI50-%s/ice-thickness.tif' % gf.glacnum)\n",
    "            if os.path.exists(debris_class_fn):\n",
    "                fn_dict['debris_class'] = debris_class_fn\n",
    "            if os.path.exists(debris_thick_fn):\n",
    "                fn_dict['debris_thick'] = debris_thick_fn\n",
    "            if os.path.exists(vx_fn):\n",
    "                fn_dict['vx'] = vx_fn\n",
    "                fn_dict['vy'] = vy_fn\n",
    "\n",
    "    if z1_date is None:\n",
    "        #Rasterize source dates\n",
    "        #Note: need to clean this up, as glac_geom_mask is not defined\n",
    "        if os.path.splitext(z1_date_fn)[1] == 'shp':\n",
    "            z1_date = get_date_a(ds_dict['z1'], z1_date_shp_lyr, glac_geom_mask, z1_datefield)\n",
    "            gf.t1 = z1_date.mean()\n",
    "        else:\n",
    "            #Otherwise, clip the timestamp array\n",
    "            fn_dict['z1_date'] = z1_date_fn\n",
    "    else:\n",
    "        gf.t1 = z1_date\n",
    "\n",
    "    if z2_date is None:\n",
    "        if os.path.splitext(z2_date_fn)[1] == 'shp':\n",
    "            z2_date = get_date_a(ds_dict['z2'], z2_date_shp_lyr, glac_geom_mask, z2_datefield)\n",
    "            gf.t1 = z2_date.mean()\n",
    "        else:\n",
    "            fn_dict['z2_date'] = z2_date_fn\n",
    "    else:\n",
    "        gf.t2 = z2_date\n",
    "\n",
    "    #Expand extent to include buffered region around glacier polygon\n",
    "    warp_extent = geolib.pad_extent(gf.glac_geom_extent, width=buff_dist)\n",
    "    if verbose:\n",
    "        print(\"Expanding extent\")\n",
    "        print(gf.glac_geom_extent)\n",
    "        print(warp_extent)\n",
    "\n",
    "    #Warp everything to common res/extent/proj\n",
    "    ds_list = warplib.memwarp_multi_fn(fn_dict.values(), res='min', \\\n",
    "            extent=warp_extent, t_srs=aea_srs, verbose=verbose, \\\n",
    "            r='cubic')\n",
    "\n",
    "    ds_dict = dict(zip(fn_dict.keys(), ds_list))\n",
    "\n",
    "    #Prepare mask for all glaciers within buffered area, not just the current glacier polygon\n",
    "    glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "    glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "    #Spatial filter\n",
    "    #glac_shp_lyr.SetSpatialFilter(geom)\n",
    "\n",
    "    #Get global glacier mask\n",
    "    #Want this to be True over ALL glacier surfaces, not just the current polygon\n",
    "    glac_shp_lyr_mask = geolib.lyr2mask(glac_shp_lyr, ds_dict['z1'])\n",
    "\n",
    "    #geom srs is not preserved when loaded from disk, attempt to reassign\n",
    "    gf.geom_srs_update()\n",
    "    #Create buffer around glacier polygon\n",
    "    glac_geom_buff = gf.glac_geom.Buffer(buff_dist)\n",
    "    #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "    glac_geom_buff_mask = geolib.geom2mask(glac_geom_buff, ds_dict['z1'])\n",
    "\n",
    "    # DEM masks\n",
    "    ds_list_masked = [iolib.ds_getma(i) for i in ds_list]\n",
    "    dem1 = np.ma.masked_less_equal(ds_list_masked[0], 0)\n",
    "    dem2 = np.ma.masked_less_equal(ds_list_masked[1], 0)\n",
    "    dems_mask = np.ma.mask_or(dem1.mask, dem2.mask)\n",
    "    # dem1 = ds_list_masked[0]\n",
    "    # dem1 = np.ma.masked_less_equal(dem1, 0)\n",
    "    # dem2 = ds_list_masked[1]\n",
    "    # dem2 = np.ma.masked_less_equal(dem2, 0)\n",
    "    # dems_mask = np.ma.mask_or(dem1.mask, dem2.mask\n",
    "\n",
    "    #Combine to identify ~1 km buffer around glacier polygon over static rock\n",
    "    static_buffer_mask = np.ma.mask_or(~glac_shp_lyr_mask, glac_geom_buff_mask)\n",
    "    static_shp_lyr_mask = np.ma.mask_or(static_buffer_mask, dems_mask)\n",
    "\n",
    "\n",
    "    if 'z1' in ds_dict:\n",
    "        #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "        glac_geom_mask = geolib.geom2mask(gf.glac_geom, ds_dict['z1'])\n",
    "        gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']))\n",
    "        #gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']), mask=glac_geom_mask)\n",
    "        print('\\n\\n# z1 pixels:', gf.z1.count(), '\\n')\n",
    "        if gf.z1.count() == 0:\n",
    "            if verbose:\n",
    "                print(\"No z1 pixels\")\n",
    "#             return None\n",
    "    else:\n",
    "        print(\"Unable to load z1 ds\")\n",
    "#         return None\n",
    "\n",
    "    if 'z2' in ds_dict:\n",
    "        gf.z2 = iolib.ds_getma(ds_dict['z2'])\n",
    "        print('\\n\\n# z2 pixels:', gf.z2.count(), '\\n')\n",
    "        if gf.z2.count() == 0:\n",
    "            if verbose:\n",
    "                print(\"No z2 pixels\")\n",
    "#             return None\n",
    "    else:\n",
    "        print(\"Unable to load z2 ds\")\n",
    "#         return None\n",
    "\n",
    "    #Apply SRTM penetration correction\n",
    "    #Do this only over glaciers, not static rock?\n",
    "    if z1_srtm_penetration_corr:\n",
    "        gf.z1 = srtm_corr(gf.z1)\n",
    "    if z2_srtm_penetration_corr:\n",
    "        gf.z2 = srtm_corr(gf.z2)\n",
    "    #gf.z2 = np.ma.array(gf.z2, mask=glac_geom_mask)\n",
    "    gf.dz = gf.z2 - gf.z1\n",
    "    if gf.dz.count() == 0:\n",
    "        if verbose:\n",
    "            print(\"No valid dz pixels\")\n",
    "#         return None\n",
    "\n",
    "    #Should add better filtering here\n",
    "    #Elevation dependent abs. threshold filter?\n",
    "\n",
    "    filter_outliers = False\n",
    "    #Remove clearly bogus pixels\n",
    "    if filter_outliers:\n",
    "        bad_perc = (0.1, 99.9)\n",
    "        #bad_perc = (1, 99)\n",
    "        rangelim = malib.calcperc(gf.dz, bad_perc)\n",
    "        gf.dz = np.ma.masked_outside(gf.dz, *rangelim)\n",
    "\n",
    "    #Preserve full dz map\n",
    "    gf.dz_full = gf.dz\n",
    "\n",
    "    #Compute stats for \"static\" surfaces (non-glacier)\n",
    "    gf.dz_static = np.ma.array(gf.dz, mask=static_shp_lyr_mask)\n",
    "    gf.dz_static_stats = malib.get_stats(gf.dz_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlacFeat' object has no attribute 'dz_full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f731b74aef24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'DZ-Full (Satellite-Map)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdz_full2plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdz_full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdz_full2plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdems_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcperc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz_full2plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz_full2plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inferno'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Elevation (m WGS84)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dem.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlacFeat' object has no attribute 'dz_full'"
     ]
    }
   ],
   "source": [
    "titles = ['DZ-Full (Satellite-Map)']\n",
    "dz_full2plot = gf.dz_full\n",
    "dz_full2plot.mask = dems_mask\n",
    "clim = malib.calcperc(dz_full2plot, (2,98))\n",
    "plot_array(dz_full2plot, clim, titles, 'inferno', 'Elevation (m WGS84)', fn='dem.png')\n",
    "\n",
    "titles = ['DZ-Static (Satellite-Map)']\n",
    "clim = malib.calcperc(gf.dz_static, (2,98))\n",
    "plot_array(gf.dz_static, clim, titles, 'inferno', 'Elevation (m WGS84)', fn='dem.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot single glacier\n",
    "rgiid = 'RGI60-' + gf.feat_fn.split('_')[0].split('.')[0].zfill(2) + '.' + gf.feat_fn.split('_')[0].split('.')[1]\n",
    "glac_shp_proj = gpd.read_file(glac_shp_proj_fn)\n",
    "glac_shp_single = glac_shp_proj[glac_shp_proj['RGIId'] == rgiid]\n",
    "glac_shp_single = glac_shp_single.reset_index()\n",
    "\n",
    "# Plot over region of interest\n",
    "ax = glac_shp_proj.plot()\n",
    "xlim = (warp_extent[0], warp_extent[2])\n",
    "ylim = (warp_extent[1], warp_extent[3])\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_title(\"UTM (m)\")\n",
    "\n",
    "ax = glac_shp_single.plot()\n",
    "xlim = (warp_extent[0], warp_extent[2])\n",
    "ylim = (warp_extent[1], warp_extent[3])\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_title(\"UTM (m)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now apply glacier mask AND mask NaN values\n",
    "glac_geom_mask = np.ma.mask_or(glac_geom_mask, dems_mask)\n",
    "nan_mask = np.ma.masked_invalid(gf.dz)\n",
    "glac_geom_mask = np.ma.mask_or(glac_geom_mask, nan_mask.mask)\n",
    "gf.z1 = np.ma.array(gf.z1, mask=glac_geom_mask)\n",
    "gf.z2 = np.ma.array(gf.z2, mask=glac_geom_mask)\n",
    "gf.dz = np.ma.array(gf.dz, mask=glac_geom_mask)\n",
    "\n",
    "gf.res = geolib.get_res(ds_dict['z1'])\n",
    "\n",
    "print('dz_count:', gf.dz.count())\n",
    "# print(gf.dz.compressed()))\n",
    "\n",
    "#Compute area covered by valid pixels in m2\n",
    "gf.valid_area = gf.dz.count() * gf.res[0] * gf.res[1]\n",
    "#Compute percentage covered by total area of polygon\n",
    "gf.valid_area_perc = 100. * (gf.valid_area / gf.glac_area)\n",
    "if verbose:\n",
    "    print('valid area %:', gf.valid_area_perc)\n",
    "\n",
    "titles = ['DZ-GLACIER (Satellite-Map)']\n",
    "clim = malib.calcperc(gf.dz, (2,98))\n",
    "plot_array(gf.dz, clim, titles, 'inferno', 'Elevation (m WGS84)', fn='dem.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gf.valid_area_perc < (100. * min_valid_area_perc):\n",
    "    if verbose:\n",
    "        print(\"Not enough valid pixels. %0.1f%% percent of glacier polygon area\" % (gf.valid_area_perc))\n",
    "#     return None\n",
    "\n",
    "else:\n",
    "    #Filter dz - throw out abs differences >150 m\n",
    "\n",
    "    #Compute dz, volume change, mass balance and stats\n",
    "    gf.z1_stats = malib.get_stats(gf.z1)\n",
    "    gf.z2_stats = malib.get_stats(gf.z2)\n",
    "\n",
    "    #Should probably take mean of z1 and z2 here\n",
    "    #For cases where WV/GE is z2, maybe best to take\n",
    "    z2_elev_med = gf.z2_stats[5]\n",
    "    #z2_elev_min = gf.z2_stats[1]\n",
    "    #z2_elev_max = gf.z2_stats[2]\n",
    "    z2_elev_min, z2_elev_max = malib.calcperc(gf.z2, (0.1, 99.9))\n",
    "    #z2_elev_p16 = gf.z2_stats[11]\n",
    "    #z2_elev_p84 = gf.z2_stats[12]\n",
    "    \n",
    "    #Caluclate stats for aspect and slope using z2\n",
    "    #Requires GDAL 2.1+\n",
    "    gf.z2_aspect = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z2'], processing='aspect', returnma=True), mask=glac_geom_mask)\n",
    "    gf.z2_aspect_stats = malib.get_stats(gf.z2_aspect)\n",
    "    z2_aspect_med = gf.z2_aspect_stats[5]\n",
    "    gf.z2_slope = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z2'], processing='slope', returnma=True), mask=glac_geom_mask)\n",
    "    gf.z2_slope_stats = malib.get_stats(gf.z2_slope)\n",
    "    z2_slope_med = gf.z2_slope_stats[5]\n",
    "\n",
    "    #Load timestamp array, if available\n",
    "    if 'z1_date' in ds_dict:\n",
    "        gf.t1 = iolib.ds_getma(ds_dict['z1_date'])\n",
    "    else:\n",
    "        if isinstance(gf.t1, datetime):\n",
    "            gf.t1 = float(timelib.dt2decyear(gf.t1))\n",
    "        #else, assume we've hardcoded decimal year\n",
    "    gf.t1_mean = np.mean(gf.t1)\n",
    "\n",
    "    if 'z2_date' in ds_dict:\n",
    "        gf.t2 = iolib.ds_getma(ds_dict['z2_date'])\n",
    "    else:\n",
    "        if isinstance(gf.t2, datetime):\n",
    "            gf.t2 = float(timelib.dt2decyear(gf.t2))\n",
    "        #else, assume we've hardcoded decimal year\n",
    "    gf.t2_mean = np.mean(gf.t2)\n",
    "\n",
    "    #These should be decimal years, either grids or constants\n",
    "    gf.dt = gf.t2 - gf.t1\n",
    "    gf.dt_mean = np.mean(gf.dt)\n",
    "    #if isinstance(gf.dt, timedelta):\n",
    "    #    gf.dt = gf.dt.total_seconds()/timelib.spy\n",
    "        \n",
    "    #Calculate dh/dt, in m/yr\n",
    "    gf.dhdt = gf.dz/gf.dt\n",
    "    gf.dhdt_sum = gf.dhdt.sum()\n",
    "    gf.dhdt_stats = malib.get_stats_dict(gf.dhdt)\n",
    "    gf.dhdt_mean = gf.dhdt_stats['mean']\n",
    "    gf.dhdt_med = gf.dhdt_stats['med']\n",
    "    gf.dhdt_nmad = gf.dhdt_stats['nmad']\n",
    "    \n",
    "    gf.dhdt_static = gf.dz_static/gf.dt\n",
    "    gf.dhdt_static_stats = malib.get_stats_dict(gf.dhdt_static)\n",
    "    gf.dhdt_static_mean = gf.dhdt_static_stats['mean']\n",
    "    gf.dhdt_static_med = gf.dhdt_static_stats['med']\n",
    "    gf.dhdt_static_nmad = gf.dhdt_static_stats['nmad']\n",
    "    \n",
    "    print('STATIC ANALYSIS DOES NOT WORK FOR UNCERTAINTY HERE BECAUSE IN 1950s THE GLACIER EXCEEDED THE RGI EXTENTS')\n",
    "    \n",
    "    #Can estimate ELA values computed from hypsometry and typical AAR\n",
    "    #For now, assume ELA is mean\n",
    "    gf.z1_ela = None\n",
    "    gf.z1_ela = gf.z1_stats[3]\n",
    "    gf.z2_ela = gf.z2_stats[3]\n",
    "    #Note: in theory, the ELA should get higher with mass loss\n",
    "    #In practice, using mean and same polygon, ELA gets lower as glacier surface thins\n",
    "    if verbose:\n",
    "        print(\"ELA(t1): %0.1f\" % gf.z1_ela)\n",
    "        print(\"ELA(t2): %0.1f\" % gf.z2_ela)\n",
    "\n",
    "    if gf.z1_ela > gf.z2_ela:\n",
    "        min_ela = gf.z2_ela\n",
    "        max_ela = gf.z1_ela\n",
    "    else:\n",
    "        min_ela = gf.z1_ela\n",
    "        max_ela = gf.z2_ela\n",
    "    \n",
    "    #Calculate uncertainty of total elevation change\n",
    "    #decorrelation length\n",
    "    L = 500\n",
    "    Acor = np.pi*L**2\n",
    "    if gf.glac_area > Acor:\n",
    "        #Correction factor for sample size area\n",
    "        Acorf = np.sqrt(Acor/(5*gf.glac_area))\n",
    "    else:\n",
    "        Acorf = 1.0\n",
    "\n",
    "    #Std or NMAD of elevation change on stable ground, assuming we know a priori uncertainty for z1 and z2\n",
    "    #dz_sigma = np.sqrt(z1_sigma**2 + z2_sigma**2)\n",
    "    #dhdt_sigma = dz_sigma/gf.dt\n",
    "\n",
    "    #This is NMAD of static pixels within buffer\n",
    "    dhdt_sigma = gf.dhdt_static_nmad\n",
    "    #Uncertainty of dh/dt\n",
    "    gf.dhdt_sigma = Acorf * (dhdt_sigma)\n",
    "\n",
    "    #This is percentage of valid pixels, 0-1\n",
    "    #p = min(gf.valid_area_perc/100., 1.0)\n",
    "    #From Brun et al, multiply uncertainty for nodata by 5x\n",
    "    #p_factor = (p + 5*(1-p))\n",
    "    p_factor = 1.0\n",
    "\n",
    "    #Calculate volume change (m3/a)\n",
    "    gf.dv = gf.dhdt_mean * gf.glac_area\n",
    "    #gf.dv = gf.dhdt_med * gf.glac_area\n",
    "    gf.dv_sum = gf.dhdt_sum*gf.res[0]*gf.res[1]\n",
    "    #print(gf.dv, gf.dv_sum, (gf.dv - gf.dv_sum))\n",
    "\n",
    "    #Volume change uncertainty (m3/a)\n",
    "    gf.dv_sigma = np.sqrt((gf.dhdt_sigma*p_factor*gf.glac_area)**2 + (area_sigma_perc * gf.glac_area)**2)\n",
    "\n",
    "    #Mass balance in mwe/a for each pixel\n",
    "    gf.mb_map = gf.dhdt * rho_is\n",
    "    gf.mb_map_sum = gf.mb_map.sum()\n",
    "    gf.mb_map_stats = malib.get_stats_dict(gf.mb_map)\n",
    "    gf.mb_map_sigma = np.ma.abs(gf.mb_map) * np.sqrt((rho_sigma/rho_is)**2 + (gf.dhdt_sigma/gf.dhdt)**2)\n",
    "    gf.mb_map_sigma_stats = malib.get_stats_dict(gf.mb_map_sigma)\n",
    "\n",
    "    #This is estimate for polygon mb in mwea\n",
    "    gf.mb_mean = gf.mb_map_stats['mean']\n",
    "    #This is average mb uncertainty, does not include area uncertainty\n",
    "    gf.mb_mean_sigma = gf.mb_map_sigma_stats['mean']\n",
    "    gf.mb_med = gf.mb_map_stats['med']\n",
    "    gf.mb_med_sigma = gf.mb_map_sigma_stats['med']\n",
    "\n",
    "    #Total mass balance for polygon in m3wea\n",
    "    #previously gf.mb_mean_totalarea\n",
    "    gf.mb_total = gf.dv * rho_is\n",
    "    gf.mb_total_sigma = np.sqrt((gf.dv_sigma*rho_is)**2 + (rho_sigma*gf.dv)**2)\n",
    "\n",
    "    \"\"\"\n",
    "    # This attempted to assign different densities above and below ELA\n",
    "    if gf.z1_ela is None:\n",
    "        gf.mb = gf.dhdt * rho_is\n",
    "    else:\n",
    "        #Initiate with average density\n",
    "        gf.mb = gf.dhdt*(rho_is + rho_f)/2.\n",
    "        #Everything that is above ELA at t2 is elevation change over firn, use firn density\n",
    "        accum_mask = (gf.z2 > gf.z2_ela).filled(0).astype(bool)\n",
    "    gf.mb[accum_mask] = (gf.dhdt*rho_f)[accum_mask]\n",
    "    #Everything that is below ELA at t1 is elevation change over ice, use ice density\n",
    "    abl_mask = (gf.z1 <= gf.z1_ela).filled(0).astype(bool)\n",
    "    gf.mb[abl_mask] = (gf.dhdt*rho_is)[abl_mask]\n",
    "    #Everything in between, use average of ice and firn density\n",
    "    #mb[(z1 > z1_ela) || (z2 <= z2_ela)] = dhdt*(rhois + rho_f)/2.\n",
    "    #Linear ramp\n",
    "    #rho_f + z2*((rho_is - rho_f)/(z2_ela - z1_ela))\n",
    "    #mb = np.where(dhdt < ela, dhdt*rho_i, dhdt*rho_s)\n",
    "    \"\"\"\n",
    "\n",
    "    #Old approach\n",
    "    #This is mb uncertainty map\n",
    "    #gf.mb_sigma = np.ma.abs(gf.mb) * np.sqrt((rho_sigma/rho_is)**2 + (gf.dhdt_sigma/gf.dhdt)**2)\n",
    "    #gf.mb_sigma_stats = malib.get_stats(gf.mb_sigma)\n",
    "    #This is average mb uncertainty\n",
    "    #gf.mb_mean_sigma = gf.mb_sigma_stats[3]\n",
    "\n",
    "    #Now calculate mb for entire polygon\n",
    "    #gf.mb_mean_totalarea = gf.mb_mean * gf.glac_area\n",
    "    #Already have area uncertainty as percentage, just use directly\n",
    "    #gf.mb_mean_totalarea_sigma = np.ma.abs(gf.mb_mean_totalarea) * np.sqrt((gf.mb_mean_sigma/gf.mb_mean)**2 + area_sigma_perc**2)\n",
    "\n",
    "    #z2_elev_med, z2_elev_min, z2_elev_max, z2_elev_p16, z2_elev_p84, \\\n",
    "    outlist = [gf.glacnum, gf.cx, gf.cy, \\\n",
    "            z2_elev_med, z2_elev_min, z2_elev_max, \\\n",
    "            z2_slope_med, z2_aspect_med, \\\n",
    "            gf.dhdt_mean, gf.dhdt_sigma, \\\n",
    "            gf.mb_mean, gf.mb_mean_sigma, \\\n",
    "            gf.glac_area, gf.mb_total, gf.mb_total_sigma, \\\n",
    "            gf.t1_mean, gf.t2_mean, gf.dt_mean, gf.valid_area_perc]\n",
    "\n",
    "    if extra_layers and (gf.glac_area_km2 > min_glac_area_writeout):\n",
    "        if 'ice_thick' in ds_dict:\n",
    "            #Load ice thickness\n",
    "            gf.H = np.ma.array(iolib.ds_getma(ds_dict['ice_thick']), mask=glac_geom_mask)\n",
    "            gf.H_mean = gf.H.mean()\n",
    "        #These should be NaN or None\n",
    "        outlist.append(gf.H_mean)\n",
    "\n",
    "        if 'debris_thick' in ds_dict:\n",
    "            gf.debris_thick = np.ma.array(iolib.ds_getma(ds_dict['debris_thick']), mask=glac_geom_mask)\n",
    "            gf.debris_thick_mean = gf.debris_thick.mean()\n",
    "        outlist.append(gf.debris_thick_mean)\n",
    "\n",
    "        if 'debris_class' in ds_dict:\n",
    "            #Load up debris cover maps\n",
    "            #Classes are: 1 = clean ice, 2 = debris, 3 = pond\n",
    "            gf.debris_class = np.ma.array(iolib.ds_getma(ds_dict['debris_class']), mask=glac_geom_mask)\n",
    "\n",
    "            #Compute debris/pond/clean percentages for entire polygon\n",
    "            if gf.debris_class.count() > 0:\n",
    "                gf.perc_clean = 100. * (gf.debris_class == 1).sum()/gf.debris_class.count()\n",
    "                gf.perc_debris = 100. * (gf.debris_class == 2).sum()/gf.debris_class.count()\n",
    "                gf.perc_pond = 100. * (gf.debris_class == 3).sum()/gf.debris_class.count()\n",
    "        outlist.extend([gf.perc_debris, gf.perc_pond, gf.perc_clean])\n",
    "\n",
    "        if 'vx' in ds_dict and 'vy' in ds_dict:\n",
    "            #Load surface velocity maps\n",
    "            gf.vx = np.ma.array(iolib.ds_getma(ds_dict['vx']), mask=glac_geom_mask)\n",
    "            gf.vy = np.ma.array(iolib.ds_getma(ds_dict['vy']), mask=glac_geom_mask)\n",
    "            gf.vm = np.ma.sqrt(gf.vx**2 + gf.vy**2)\n",
    "            gf.vm_mean = gf.vm.mean()\n",
    "\n",
    "            if gf.H is not None:\n",
    "                #Compute flux\n",
    "                gf.Q = gf.H * v_col_f * np.array([gf.vx, gf.vy])\n",
    "                #Note: np.gradient returns derivatives relative to axis number, so (y, x) in this case\n",
    "                #Want x-derivative of x component\n",
    "                gf.divQ = np.gradient(gf.Q[0])[1] + np.gradient(gf.Q[1])[0]\n",
    "\n",
    "                #gf.divQ = gf.H*(np.gradient(v_col_f*gf.vx)[1] + np.gradient(v_col_f*gf.vy)[0]) \\\n",
    "                        #+ v_col_f*gf.vx*(np.gradient(gf.H)[1]) + v_col_f*gf.vy*(np.gradient(gf.H)[0])\n",
    "\n",
    "                #Should smooth divQ, better handling of data gaps\n",
    "        outlist.append(gf.vm_mean)\n",
    "\n",
    "    if verbose:\n",
    "        print('Area [km2]:', gf.glac_area / 1e6)\n",
    "        print('Mean mb: %0.2f +/- %0.2f mwe/yr' % (gf.mb_mean, gf.mb_mean_sigma))\n",
    "        print('Sum/Area mb: %0.2f mwe/yr' % (gf.mb_total/gf.glac_area))\n",
    "        print('Mean mb * Area: %0.2f +/- %0.2f m3we/yr' % (gf.mb_total, gf.mb_total_sigma))\n",
    "#         print('Sum mb: %0.2f m3we/yr' % gf.mb_total)\n",
    "        print('-------------------------------')\n",
    "    \n",
    "    #Write out mb stats for entire polygon - in case processing is interupted\n",
    "    #out = np.array(outlist, dtype=float)\n",
    "    out = np.full(len(out_fmt), np.nan)\n",
    "    out[0:len(outlist)] = np.array(outlist, dtype=float)\n",
    "    #Note, need a 2D array here, add 0 axis\n",
    "\n",
    "    print(out)\n",
    "\n",
    "    np.savetxt(out_csv_fn, out[np.newaxis,:], fmt=out_fmt, delimiter=',', header=out_header, comments='')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do AED for all\n",
    "#Compute mb using scaled AED vs. polygon\n",
    "if mb_plot and (gf.glac_area_km2 > min_glac_area_writeout):\n",
    "    dz_clim = (-2.0, 2.0)\n",
    "    z_bin_edges = hist_plot(gf, outdir, bin_width=bin_width, dz_clim=dz_clim)\n",
    "    gf.z1_hs = geolib.gdaldem_mem_ds(ds_dict['z1'], processing='hillshade', returnma=True)\n",
    "    gf.z2_hs = geolib.gdaldem_mem_ds(ds_dict['z2'], processing='hillshade', returnma=True)\n",
    "    map_plot(gf, z_bin_edges, outdir, dz_clim=dz_clim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rasterenv]",
   "language": "python",
   "name": "conda-env-rasterenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
